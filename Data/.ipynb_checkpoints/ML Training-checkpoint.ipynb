{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53f3b1c",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b951f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "67e7edac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/srj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n",
    "#df0=pd.read_csv(\"data.csv\")\n",
    "df = pd.read_csv(\"data_2.csv\")\n",
    "#df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "740ae128",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d13bf4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    no_punctuation = ''.join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    stems = [ps.lemmatize(word) for word in tokens if word not in stopwords] # Remove Stopwords\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ff5f9bf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4846), (1, 4893), (2, 4950)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(Counter(df['LABEL']).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "943e74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['TEXT']\n",
    "Y=df['LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "61890dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14689,)\n",
      "(14689,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f2cacabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = 6042\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X[:split_index], Y[:split_index],random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "55100f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[split_index:]\n",
    "Y_train = Y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5fbcb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(analyzer=clean_text)\n",
    "vector=vect.fit_transform(X_train)\n",
    "X_train=pd.DataFrame(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "74988076",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(analyzer=clean_text)\n",
    "vector=vect.fit_transform(X_test)\n",
    "X_test=pd.DataFrame(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "50c91c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0     1     2     3     4     5     6     7     8     9     ...  5796  \\\n",
      "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "\n",
      "   5797  5798  5799  5800  5801  5802  5803  5804  5805  \n",
      "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "\n",
      "[5 rows x 5806 columns]\n",
      "       0     1     2     3     4     5     6     7     8     9     ...  5333  \\\n",
      "0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "1  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "2  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "3  0.092948   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "4  0.107214   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
      "\n",
      "   5334  5335  5336  5337  5338  5339  5340  5341      5342  \n",
      "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  \n",
      "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  \n",
      "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  \n",
      "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.000000  \n",
      "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.366239  \n",
      "\n",
      "[5 rows x 5343 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6b3c2130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X = [str(x) for x in X] #converting to iterable object before passing to fit transform\n",
    "#vect = TfidfVectorizer(analyzer=clean_text)\n",
    "#vector=vect.fit_transform(X)\n",
    "#X=pd.DataFrame(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7c72d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14689,)\n",
      "(14689,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 5343 features, but LogisticRegression is expecting 5806 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(X_train,Y_train)\n\u001b[0;32m----> 5\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m pred_prob\u001b[38;5;241m=\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39maccuracy_score(Y_test, predicted))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 419\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    421\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/linear_model/_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    397\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    398\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 400\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mreshape(scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 5343 features, but LogisticRegression is expecting 5806 features as input."
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "classifier.fit(X_train,Y_train)\n",
    "predicted = classifier.predict(X_test)\n",
    "pred_prob=classifier.predict_proba(X_test)\n",
    "print(metrics.accuracy_score(Y_test, predicted))\n",
    "print(metrics.roc_auc_score(Y_test,pred_prob,multi_class='ovr'))\n",
    "print(metrics.f1_score(Y_test,predicted,average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f034ad",
   "metadata": {},
   "source": [
    "# Word 2 VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eefc5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [text.split() for text in X_train]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3bb5961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = []\n",
    "for sentence in sentences:\n",
    "    sentence_vectors = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            sentence_vectors.append(model.wv[word])\n",
    "    if sentence_vectors:\n",
    "        X_train_w2v.append(np.mean(sentence_vectors, axis=0))\n",
    "    else:\n",
    "        X_train_w2v.append(np.zeros(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5571ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srj/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_w2v, Y_train)\n",
    "X_train_w2v = np.array(X_train_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6c657357",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'Word2Vec' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m vecs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m:\n\u001b[1;32m     10\u001b[0m         vecs\u001b[38;5;241m.\u001b[39mappend(w2v_model[word])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vecs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'Word2Vec' is not iterable"
     ]
    }
   ],
   "source": [
    "x_test_w2v = np.zeros((len(X_test), model.vector_size))\n",
    "\n",
    "for i, doc in enumerate(X_test):\n",
    "    # Split the text document into words\n",
    "    words = doc.split()\n",
    "    # Compute the average vector of the words in the document\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            vecs.append(model[word])\n",
    "    if len(vecs) > 0:\n",
    "        avg_vec = np.mean(vecs, axis=0)\n",
    "    else:\n",
    "        avg_vec = np.zeros(model.vector_size)\n",
    "    x_test_w2v[i] = avg_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "725033a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mx_test_w2v\u001b[49m)\n\u001b[1;32m      2\u001b[0m acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y_pred \u001b[38;5;241m==\u001b[39m y_test)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test_w2v)\n",
    "acc = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
